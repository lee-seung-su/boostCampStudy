# Day3-크롤링-학습정리

**콘솔 응용 프로그램**

GUI가 없는 응용프로그램으로 키보드 명령만으로 작동

즉 콘솔 창을 기반으로 작동하는 프로그램

ex) Linux의 shell, Windows의 명령 프롬프트

**콘솔**

입력장치와 출력장치의 총칭

컴퓨터의 대표적 콘솔 : 키보드 + 모니터

주로 명령 내리고 출력 볼 수 있는 창 = 콘솔창

**크롤링**

웹사이트의 페이지 브라우징

**스크래핑**

원하는 데이터 추출

크롤링, 스크래핑 비슷한 개념으로 혼용된다

## **크롤링 매커니즘**

대상 선정 → 데이터 로드 → 데이터 분석 → 데이터 수집

**대상 선정** : 대상은 웹이나 정적문서, api같은 서비스가 될 수도 있다

**데이터 로드** : URL로 데이터 요청하고 데이터 가져오기

**데이터 분석** : 라이브러리 이용해서 태그별로 데이터 뽑아내는 것

**데이터 수집** : 데이터가지고 작업

웹페이지 : HTML문서로 작성

즉 HTML태그를 찾아서 원하는 데이터 추출

**URL** : 웹주소 (Uniform Resource Locator)

컴퓨터 네트워크 상에서 리소스가 어디 있는지 알려주기 위한 규약

ex) [http://www.school.com/grade/](http://www.school.com/grade/2)

**URI** : 리소스 식별자 (Uniform Resource Identifier)

웹에서 사용하는 논리적 또는 물리적 리소스를 식별하는 문자열

ex) [http://www.school.com/grade/](http://www.school.com/grade/2)**2**

**URN** : 리소스 이름(Uniform Resource Name)

URL이 특정시점의 특정 자료의 위치라면 URL을 바꾸면 변경전 URL로 해당자료 찾아갈 수 없다. 즉 URL의 단점보완위해 나옴

ex) urn : uuid : 6e8bc430-9c3a-11d9-9669-0800200c9a66 (범용 고유 식별자

![Day3-%E1%84%8F%E1%85%B3%E1%84%85%E1%85%A9%E1%86%AF%E1%84%85%E1%85%B5%E1%86%BC-%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2090630b06aec847c8a1a44e5820e7c8ff/Untitled.png](Day3-%E1%84%8F%E1%85%B3%E1%84%85%E1%85%A9%E1%86%AF%E1%84%85%E1%85%B5%E1%86%BC-%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%2090630b06aec847c8a1a44e5820e7c8ff/Untitled.png)

**robots.txt**

웹페이지를 크롤링 때 허용범위, 허용 여부 등을 정해놓은 텍스트파일

검색 로봇으로 크롤링 시 해당 규칙을 따라야한다

 

**Jsoup**

HTML 파싱 라이브러리

html형식의 string을 java로 넘겨주면 java에서 사용가능한 DOM객체로 만들어주는 parser

**jsoup 주요 요소**

![Untitled](https://user-images.githubusercontent.com/52225690/126525900-b9ec2c93-5ad9-43cf-9b7a-89edb20abecb.png)

**사용법**

val crawlURL = "[https://www.google.com/search?q=${keyWord}](https://www.google.com/search?q=$%7BkeyWord%7D)"

val doc: document = jsoup.connect(crawlURL).get()  //crawlURL안의 HTML 전체문서를 Document객체로  가져온다

이후 document.select(CssQuery)로 데이터 파싱

DOM = Document객체 (Document Object Model)

  

**LRU (Least Recently Used)**

- 페이지 교체 알고리즘
- 페이지에서 제거할 때 가장 오랫동안 사용되지 않을 것을 제거
- 주로 큐로 구현

다른 **페이지 교체 알고리즘 = Cache Replacement 정책**

- FIFO (first in first out) =큐
- OPT (optimal Page Replacement) - 사용되지 않을 것 먼저 제거(예측샷)
- LFU (Least Frequently Used) - 가장 적은 참조횟수 교체
- MFU (Most Frequently Used) - 가장 많이 참조된것 교체 → 앞으로 사용률 낮을것이라 판단
